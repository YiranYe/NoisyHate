{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import time\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "from config import HUGGINGFACE_API_TOKEN, PERSPECTIVE_API_KEY\n",
    "\n",
    "with open('../../data/huggingface_cache.json') as f:\n",
    "  huggingface_cache = f.read()\n",
    "huggingface_cache_list = json.loads(huggingface_cache)\n",
    "huggingface_cache = {}\n",
    "for item in huggingface_cache_list:\n",
    "    huggingface_cache[tuple(item[0])] = item[1]\n",
    "\n",
    "def huggingface_predict(row,col_name, model_name, label_set, positive_label):\n",
    "    sentence = row[col_name]\n",
    "    if (model_name,sentence) not in huggingface_cache:\n",
    "        time.sleep(2)\n",
    "        API_URL = \"https://api-inference.huggingface.co/models/\" + model_name\n",
    "        API_TOKEN = HUGGINGFACE_API_TOKEN\n",
    "        headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
    "        data = json.dumps(sentence)\n",
    "        response = requests.request(\"POST\", API_URL, headers=headers, data=data)\n",
    "        res = json.loads(response.content.decode(\"utf-8\"))\n",
    "        print(res)\n",
    "        while 'error' in res or len(res[0]) < 2 or res[0][1]['label'] not in label_set:\n",
    "            print(sentence, res)\n",
    "            time.sleep(30)\n",
    "            response = requests.request(\"POST\", API_URL, headers=headers, data=data)\n",
    "            res = json.loads(response.content.decode(\"utf-8\"))\n",
    "        huggingface_cache[(model_name,sentence)] = res[0][0]['score'] if res[0][0]['label'] == positive_label else res[0][1]['score'] \n",
    "    return huggingface_cache[(model_name,sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient import discovery\n",
    "\n",
    "with open('../../data/perspective_cache.json') as f:\n",
    "  perspective_cache_f = f.read()\n",
    "perspective_cache_list = json.loads(perspective_cache_f)\n",
    "perspective_cache = {}\n",
    "for item in perspective_cache_list:\n",
    "    perspective_cache[item[0]] = item[1]\n",
    "\n",
    "\n",
    "API_KEY = PERSPECTIVE_API_KEY\n",
    "\n",
    "client = discovery.build(\n",
    "    \"commentanalyzer\",\n",
    "    \"v1alpha1\",\n",
    "    developerKey=API_KEY,\n",
    "    discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
    "    static_discovery=False,\n",
    ")\n",
    "def perspective_api(row, t=0.7,col_name='Answer.perturbed'):\n",
    "    sentence = row[col_name]\n",
    "    if not sentence in perspective_cache:\n",
    "        time.sleep(2)\n",
    "        analyze_request = {\n",
    "                    'comment': { 'text': sentence },\n",
    "                    'requestedAttributes': {'TOXICITY': {}},\n",
    "                    'languages': [\"en\"],\n",
    "                    'doNotStore':True\n",
    "                }\n",
    "        \n",
    "        response = client.comments().analyze(body=analyze_request).execute()\n",
    "        perspective_cache[sentence] = response[\"attributeScores\"][\"TOXICITY\"][\"spanScores\"][0][\"score\"][\"value\"]\n",
    "        print(sentence, perspective_cache[sentence])\n",
    "    return perspective_cache[sentence]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "grouped_multiple = pd.read_csv(\"../../toxic detection model test set with perturbations.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(174, 51)\n",
      "(471, 51)\n",
      "(207, 51)\n",
      "(523, 51)\n",
      "(210, 51)\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import words\n",
    "import Levenshtein\n",
    "\n",
    "place_holder_list = ['_','-','*','~','/']\n",
    "\n",
    "\n",
    "word_set = set(words.words())\n",
    "\n",
    "def classify_perturbation_type(row, func):\n",
    "    c,p = row['clean_version'].split()[row['location']], row['pert_word']\n",
    "    return 1 if func(c,p) else 0\n",
    "\n",
    "\n",
    "\n",
    "def locate_pert(row):\n",
    "    # print(row)\n",
    "    c,p = row['clean_version'].split(), row['perturbed_version'].split()\n",
    "    for i in range(len(c)):\n",
    "        if c[i] != p[i]:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "\n",
    "def distance(row):\n",
    "    l = row['location']\n",
    "    c,p = row['clean_version'].split()[l], row['perturbed_version'].split()[l]\n",
    "    return Levenshtein.distance(c.lower(),p.lower())\n",
    "\n",
    "\n",
    "def show_pert(row):\n",
    "    l = row['location']\n",
    "    return row['perturbed_version'].split()[l]\n",
    "\n",
    "\n",
    "def simplify_word(word):\n",
    "    pw = list(word.lower())\n",
    "    i = 1\n",
    "    while i < len(pw):\n",
    "        if pw[i] == pw[i-1]:\n",
    "            pw.pop(i)\n",
    "        else:\n",
    "            i += 1\n",
    "    return ''.join(pw)\n",
    "\n",
    "\n",
    "def lowercase_uppercase(clean_word, pert_word):\n",
    "    return pert_word[1:].lower() != pert_word[1:] and pert_word.upper() != pert_word\n",
    "\n",
    "\n",
    "def interesting_lowercase_uppercase(clean_word, pert_word):\n",
    "    collect = ''\n",
    "    for c in pert_word:\n",
    "        if c.lower() != c:\n",
    "            collect += c\n",
    "    return pert_word.lower() != pert_word and collect.lower() in word_set and len(collect) != len(pert_word) and len(collect) > 1\n",
    "\n",
    "\n",
    "def repeat_char(clean_word, pert_word):\n",
    "    c,p = clean_word.lower(),pert_word.lower()\n",
    "    simp_c, simp_p = simplify_word(c), simplify_word(p)\n",
    "    return len(p) > len(c) and simp_c == simp_p and simp_p != p\n",
    "\n",
    "\n",
    "def abbr(clean_word, pert_word):\n",
    "    c,p = clean_word.lower(),pert_word.lower()\n",
    "    i = 0\n",
    "    for character in c:\n",
    "        if character == p[i]:\n",
    "            i += 1\n",
    "            if i == len(p):\n",
    "                return len(p) < len(c)\n",
    "    return False\n",
    "\n",
    "\n",
    "def placeholder(self, clean_word, pert_word):\n",
    "    if len(pert_word) < 4:\n",
    "        return False\n",
    "    for c in list(pert_word):\n",
    "        if c in self.place_holder_list:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def special_character(clean_word, pert_word):\n",
    "    for c in list(pert_word.lower()):\n",
    "        if (c < 'a' or c > 'z') and c not in place_holder_list:\n",
    "            return True\n",
    "    return False\n",
    "grouped_multiple['location'] = grouped_multiple.apply(locate_pert,axis=1)\n",
    "# grouped_multiple['distance'] = grouped_multiple.apply(distance,axis=1)\n",
    "grouped_multiple['pert_word'] = grouped_multiple.apply(show_pert,axis=1)\n",
    "grouped_multiple['is_special'] = grouped_multiple.apply(classify_perturbation_type, args=(special_character,), axis=1)\n",
    "grouped_multiple['is_repeat'] = grouped_multiple.apply(classify_perturbation_type, args=(repeat_char,), axis=1)\n",
    "grouped_multiple['is_interesting_up'] = grouped_multiple.apply(classify_perturbation_type, args=(interesting_lowercase_uppercase,), axis=1)\n",
    "grouped_multiple['is_up'] = grouped_multiple.apply(classify_perturbation_type, args=(lowercase_uppercase,), axis=1)\n",
    "grouped_multiple['is_abbr'] = grouped_multiple.apply(classify_perturbation_type, args=(abbr,), axis=1)\n",
    "\n",
    "print(grouped_multiple[grouped_multiple[\"is_special\"] == 1].shape)\n",
    "print(grouped_multiple[grouped_multiple[\"is_repeat\"] == 1].shape)\n",
    "print(grouped_multiple[grouped_multiple[\"is_interesting_up\"] == 1].shape)\n",
    "print(grouped_multiple[grouped_multiple[\"is_up\"] == 1].shape)\n",
    "print(grouped_multiple[grouped_multiple[\"is_abbr\"] == 1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perspective API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thats an Assinine comment 0.32128486\n",
      "we just got sCrEWeD 0.47473195\n",
      "thats an asinine comment 0.45921504\n",
      "we just got screwed 0.50503236\n"
     ]
    }
   ],
   "source": [
    "grouped_multiple['perspective_result_on_pert'] = grouped_multiple.apply(perspective_api, args=(0.5,'perturbed_version'), axis=1)\n",
    "grouped_multiple['perspective_result_on_clean'] = grouped_multiple.apply(perspective_api, args=(0.5,'clean_version'), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Model cardiffnlp/bertweet-base-offensive is currently loading', 'estimated_time': 20.0}\n",
      "thats an asinine comment {'error': 'Model cardiffnlp/bertweet-base-offensive is currently loading', 'estimated_time': 20.0}\n",
      "thats an asinine comment {'error': 'Service Unavailable'}\n",
      "[[{'label': 'LABEL_1', 'score': 0.7726978063583374}, {'label': 'LABEL_0', 'score': 0.22730223834514618}]]\n",
      "clean acc:  0.8976848394324123\n",
      "[[{'label': 'LABEL_0', 'score': 0.7121782302856445}, {'label': 'LABEL_1', 'score': 0.2878217101097107}]]\n",
      "[[{'label': 'LABEL_0', 'score': 0.729638397693634}, {'label': 'LABEL_1', 'score': 0.27036160230636597}]]\n",
      "perturb acc:  0.5571321882001493\n"
     ]
    }
   ],
   "source": [
    "model_name = 'cardiffnlp/bertweet-base-offensive'\n",
    "label_set = set(['LABEL_0', 'LABEL_1'])\n",
    "pos_label = 'LABEL_1'\n",
    "\n",
    "grouped_multiple[model_name + '_on_clean'] = grouped_multiple.apply(huggingface_predict, args=('clean_version',model_name,label_set,pos_label,), axis=1)\n",
    "print(\"clean acc: \", grouped_multiple[grouped_multiple[model_name + '_on_clean'] > 0.5].shape[0] / grouped_multiple.shape[0])\n",
    "grouped_multiple[model_name + '_on_perturb'] = grouped_multiple.apply(huggingface_predict, args=('perturbed_version',model_name,label_set,pos_label,), axis=1)\n",
    "print(\"perturb acc: \", grouped_multiple[grouped_multiple[model_name + '_on_perturb'] > 0.5].shape[0] / grouped_multiple.shape[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Model cardiffnlp/roberta-base-offensive is currently loading', 'estimated_time': 20.0}\n",
      "thats an asinine comment {'error': 'Model cardiffnlp/roberta-base-offensive is currently loading', 'estimated_time': 20.0}\n",
      "[[{'label': 'offensive', 'score': 0.9631127715110779}, {'label': 'non-offensive', 'score': 0.03688724339008331}]]\n",
      "clean acc:  0.9156086631814787\n",
      "[[{'label': 'non-offensive', 'score': 0.8302836418151855}, {'label': 'offensive', 'score': 0.16971634328365326}]]\n",
      "[[{'label': 'non-offensive', 'score': 0.9694981575012207}, {'label': 'offensive', 'score': 0.030501825734972954}]]\n",
      "perturb acc:  0.5212845407020165\n"
     ]
    }
   ],
   "source": [
    "model_name = 'cardiffnlp/roberta-base-offensive'\n",
    "label_set = set(['offensive', 'non-offensive'])\n",
    "pos_label = 'offensive'\n",
    "\n",
    "grouped_multiple[model_name + '_on_clean'] = grouped_multiple.apply(huggingface_predict, args=('clean_version',model_name,label_set,pos_label,), axis=1)\n",
    "print(\"clean acc: \", grouped_multiple[grouped_multiple[model_name + '_on_clean'] > 0.5].shape[0] / grouped_multiple.shape[0])\n",
    "grouped_multiple[model_name + '_on_perturb'] = grouped_multiple.apply(huggingface_predict, args=('perturbed_version',model_name,label_set,pos_label,), axis=1)\n",
    "print(\"perturb acc: \", grouped_multiple[grouped_multiple[model_name + '_on_perturb'] > 0.5].shape[0] / grouped_multiple.shape[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_special\n",
      "0.0 1.0 1.0\n",
      "0.01 1.0 1.0\n",
      "0.02 1.0 1.0\n",
      "0.03 0.994 1.0\n",
      "0.04 0.994 1.0\n",
      "0.05 0.994 1.0\n",
      "0.06 0.994 1.0\n",
      "0.07 0.989 1.0\n",
      "0.08 0.989 1.0\n",
      "0.09 0.989 1.0\n",
      "0.1 0.989 1.0\n",
      "0.11 0.983 1.0\n",
      "0.12 0.977 1.0\n",
      "0.13 0.977 1.0\n",
      "0.14 0.971 1.0\n",
      "0.15 0.971 1.0\n",
      "0.16 0.971 1.0\n",
      "0.17 0.971 1.0\n",
      "0.18 0.96 1.0\n",
      "0.19 0.96 1.0\n",
      "0.2 0.96 1.0\n",
      "0.21 0.96 1.0\n",
      "0.22 0.96 1.0\n",
      "0.23 0.96 1.0\n",
      "0.24 0.96 1.0\n",
      "0.25 0.96 1.0\n",
      "0.26 0.948 1.0\n",
      "0.27 0.948 1.0\n",
      "0.28 0.943 1.0\n",
      "0.29 0.925 1.0\n",
      "0.3 0.925 1.0\n",
      "0.31 0.902 1.0\n",
      "0.32 0.902 1.0\n",
      "0.33 0.897 1.0\n",
      "0.34 0.874 0.994\n",
      "0.35 0.874 0.994\n",
      "0.36 0.868 0.994\n",
      "0.37 0.851 0.983\n",
      "0.38 0.816 0.983\n",
      "0.39 0.81 0.977\n",
      "0.4 0.81 0.96\n",
      "0.41 0.782 0.954\n",
      "0.42 0.77 0.954\n",
      "0.43 0.753 0.943\n",
      "0.44 0.724 0.931\n",
      "0.45 0.695 0.92\n",
      "0.46 0.649 0.902\n",
      "0.47 0.649 0.897\n",
      "0.48 0.609 0.856\n",
      "0.49 0.575 0.833\n",
      "0.5 0.563 0.805\n",
      "0.51 0.506 0.782\n",
      "0.52 0.477 0.753\n",
      "0.53 0.454 0.713\n",
      "0.54 0.454 0.707\n",
      "0.55 0.431 0.695\n",
      "0.56 0.402 0.667\n",
      "0.57 0.397 0.649\n",
      "0.58 0.351 0.615\n",
      "0.59 0.322 0.58\n",
      "0.6 0.31 0.58\n",
      "0.61 0.259 0.506\n",
      "0.62 0.247 0.506\n",
      "0.63 0.241 0.483\n",
      "0.64 0.241 0.454\n",
      "0.65 0.213 0.437\n",
      "0.66 0.213 0.402\n",
      "0.67 0.201 0.385\n",
      "0.68 0.195 0.379\n",
      "0.69 0.161 0.328\n",
      "0.7 0.161 0.316\n",
      "0.71 0.161 0.316\n",
      "0.72 0.149 0.282\n",
      "0.73 0.132 0.282\n",
      "0.74 0.126 0.282\n",
      "0.75 0.115 0.276\n",
      "0.76 0.109 0.259\n",
      "0.77 0.092 0.224\n",
      "0.78 0.092 0.213\n",
      "0.79 0.075 0.19\n",
      "0.8 0.075 0.178\n",
      "0.81 0.075 0.178\n",
      "0.82 0.075 0.161\n",
      "0.83 0.046 0.155\n",
      "0.84 0.029 0.138\n",
      "0.85 0.023 0.121\n",
      "0.86 0.011 0.092\n",
      "0.87 0.0 0.052\n",
      "0.88 0.0 0.046\n",
      "0.89 0.0 0.04\n",
      "0.9 0.0 0.04\n",
      "0.91 0.0 0.011\n",
      "0.92 0.0 0.006\n",
      "0.93 0.0 0.0\n",
      "0.94 0.0 0.0\n",
      "0.95 0.0 0.0\n",
      "0.96 0.0 0.0\n",
      "0.97 0.0 0.0\n",
      "0.98 0.0 0.0\n",
      "0.99 0.0 0.0\n",
      "1.0 0.0 0.0\n",
      "is_repeat\n",
      "0.0 1.0 1.0\n",
      "0.01 1.0 1.0\n",
      "0.02 1.0 1.0\n",
      "0.03 1.0 1.0\n",
      "0.04 0.998 1.0\n",
      "0.05 0.998 1.0\n",
      "0.06 0.998 1.0\n",
      "0.07 0.998 1.0\n",
      "0.08 0.998 1.0\n",
      "0.09 0.998 1.0\n",
      "0.1 0.998 1.0\n",
      "0.11 0.998 1.0\n",
      "0.12 0.996 1.0\n",
      "0.13 0.996 1.0\n",
      "0.14 0.996 1.0\n",
      "0.15 0.992 1.0\n",
      "0.16 0.992 1.0\n",
      "0.17 0.992 1.0\n",
      "0.18 0.992 1.0\n",
      "0.19 0.992 1.0\n",
      "0.2 0.989 1.0\n",
      "0.21 0.987 1.0\n",
      "0.22 0.987 1.0\n",
      "0.23 0.987 1.0\n",
      "0.24 0.985 1.0\n",
      "0.25 0.983 1.0\n",
      "0.26 0.972 0.996\n",
      "0.27 0.97 0.996\n",
      "0.28 0.966 0.996\n",
      "0.29 0.96 0.996\n",
      "0.3 0.96 0.996\n",
      "0.31 0.958 0.994\n",
      "0.32 0.953 0.994\n",
      "0.33 0.947 0.994\n",
      "0.34 0.934 0.989\n",
      "0.35 0.934 0.989\n",
      "0.36 0.932 0.987\n",
      "0.37 0.93 0.985\n",
      "0.38 0.919 0.979\n",
      "0.39 0.913 0.975\n",
      "0.4 0.892 0.97\n",
      "0.41 0.875 0.958\n",
      "0.42 0.873 0.958\n",
      "0.43 0.841 0.945\n",
      "0.44 0.832 0.938\n",
      "0.45 0.809 0.93\n",
      "0.46 0.779 0.907\n",
      "0.47 0.754 0.896\n",
      "0.48 0.735 0.849\n",
      "0.49 0.711 0.841\n",
      "0.5 0.701 0.832\n",
      "0.51 0.656 0.8\n",
      "0.52 0.628 0.764\n",
      "0.53 0.605 0.743\n",
      "0.54 0.599 0.73\n",
      "0.55 0.586 0.709\n",
      "0.56 0.546 0.675\n",
      "0.57 0.524 0.654\n",
      "0.58 0.48 0.628\n",
      "0.59 0.452 0.586\n",
      "0.6 0.425 0.569\n",
      "0.61 0.378 0.512\n",
      "0.62 0.372 0.501\n",
      "0.63 0.335 0.48\n",
      "0.64 0.304 0.45\n",
      "0.65 0.282 0.401\n",
      "0.66 0.261 0.359\n",
      "0.67 0.242 0.35\n",
      "0.68 0.231 0.342\n",
      "0.69 0.191 0.282\n",
      "0.7 0.185 0.276\n",
      "0.71 0.185 0.276\n",
      "0.72 0.157 0.24\n",
      "0.73 0.146 0.221\n",
      "0.74 0.136 0.208\n",
      "0.75 0.127 0.193\n",
      "0.76 0.115 0.163\n",
      "0.77 0.096 0.151\n",
      "0.78 0.093 0.136\n",
      "0.79 0.059 0.117\n",
      "0.8 0.059 0.106\n",
      "0.81 0.059 0.106\n",
      "0.82 0.059 0.102\n",
      "0.83 0.053 0.087\n",
      "0.84 0.045 0.074\n",
      "0.85 0.038 0.064\n",
      "0.86 0.019 0.034\n",
      "0.87 0.017 0.025\n",
      "0.88 0.013 0.023\n",
      "0.89 0.011 0.023\n",
      "0.9 0.008 0.013\n",
      "0.91 0.004 0.006\n",
      "0.92 0.002 0.002\n",
      "0.93 0.0 0.002\n",
      "0.94 0.0 0.0\n",
      "0.95 0.0 0.0\n",
      "0.96 0.0 0.0\n",
      "0.97 0.0 0.0\n",
      "0.98 0.0 0.0\n",
      "0.99 0.0 0.0\n",
      "1.0 0.0 0.0\n",
      "is_interesting_up\n",
      "0.0 1.0 1.0\n",
      "0.01 1.0 1.0\n",
      "0.02 1.0 1.0\n",
      "0.03 1.0 1.0\n",
      "0.04 1.0 1.0\n",
      "0.05 1.0 1.0\n",
      "0.06 1.0 1.0\n",
      "0.07 1.0 1.0\n",
      "0.08 1.0 1.0\n",
      "0.09 1.0 1.0\n",
      "0.1 1.0 1.0\n",
      "0.11 1.0 1.0\n",
      "0.12 1.0 1.0\n",
      "0.13 1.0 1.0\n",
      "0.14 1.0 1.0\n",
      "0.15 1.0 1.0\n",
      "0.16 1.0 1.0\n",
      "0.17 1.0 1.0\n",
      "0.18 1.0 1.0\n",
      "0.19 1.0 1.0\n",
      "0.2 1.0 1.0\n",
      "0.21 1.0 1.0\n",
      "0.22 1.0 1.0\n",
      "0.23 1.0 1.0\n",
      "0.24 1.0 1.0\n",
      "0.25 1.0 1.0\n",
      "0.26 1.0 1.0\n",
      "0.27 1.0 1.0\n",
      "0.28 0.995 1.0\n",
      "0.29 0.995 1.0\n",
      "0.3 0.995 1.0\n",
      "0.31 0.995 1.0\n",
      "0.32 0.995 1.0\n",
      "0.33 0.99 0.995\n",
      "0.34 0.981 0.99\n",
      "0.35 0.981 0.99\n",
      "0.36 0.981 0.99\n",
      "0.37 0.976 0.986\n",
      "0.38 0.971 0.986\n",
      "0.39 0.971 0.981\n",
      "0.4 0.966 0.981\n",
      "0.41 0.932 0.976\n",
      "0.42 0.932 0.971\n",
      "0.43 0.908 0.952\n",
      "0.44 0.908 0.942\n",
      "0.45 0.884 0.932\n",
      "0.46 0.86 0.903\n",
      "0.47 0.841 0.889\n",
      "0.48 0.792 0.841\n",
      "0.49 0.773 0.831\n",
      "0.5 0.749 0.797\n",
      "0.51 0.725 0.754\n",
      "0.52 0.715 0.729\n",
      "0.53 0.681 0.72\n",
      "0.54 0.676 0.7\n",
      "0.55 0.657 0.686\n",
      "0.56 0.623 0.647\n",
      "0.57 0.618 0.618\n",
      "0.58 0.58 0.58\n",
      "0.59 0.522 0.541\n",
      "0.6 0.517 0.522\n",
      "0.61 0.469 0.469\n",
      "0.62 0.464 0.459\n",
      "0.63 0.425 0.444\n",
      "0.64 0.406 0.411\n",
      "0.65 0.357 0.386\n",
      "0.66 0.329 0.367\n",
      "0.67 0.319 0.353\n",
      "0.68 0.314 0.348\n",
      "0.69 0.251 0.304\n",
      "0.7 0.242 0.29\n",
      "0.71 0.242 0.29\n",
      "0.72 0.222 0.271\n",
      "0.73 0.203 0.246\n",
      "0.74 0.184 0.242\n",
      "0.75 0.184 0.237\n",
      "0.76 0.169 0.213\n",
      "0.77 0.159 0.193\n",
      "0.78 0.15 0.188\n",
      "0.79 0.116 0.15\n",
      "0.8 0.106 0.135\n",
      "0.81 0.106 0.135\n",
      "0.82 0.092 0.126\n",
      "0.83 0.082 0.097\n",
      "0.84 0.068 0.087\n",
      "0.85 0.048 0.072\n",
      "0.86 0.034 0.043\n",
      "0.87 0.024 0.039\n",
      "0.88 0.024 0.034\n",
      "0.89 0.019 0.029\n",
      "0.9 0.014 0.014\n",
      "0.91 0.005 0.005\n",
      "0.92 0.0 0.0\n",
      "0.93 0.0 0.0\n",
      "0.94 0.0 0.0\n",
      "0.95 0.0 0.0\n",
      "0.96 0.0 0.0\n",
      "0.97 0.0 0.0\n",
      "0.98 0.0 0.0\n",
      "0.99 0.0 0.0\n",
      "1.0 0.0 0.0\n",
      "is_up\n",
      "0.0 1.0 1.0\n",
      "0.01 1.0 1.0\n",
      "0.02 1.0 1.0\n",
      "0.03 1.0 1.0\n",
      "0.04 1.0 1.0\n",
      "0.05 1.0 1.0\n",
      "0.06 1.0 1.0\n",
      "0.07 1.0 1.0\n",
      "0.08 1.0 1.0\n",
      "0.09 1.0 1.0\n",
      "0.1 1.0 1.0\n",
      "0.11 1.0 1.0\n",
      "0.12 0.996 1.0\n",
      "0.13 0.996 1.0\n",
      "0.14 0.996 1.0\n",
      "0.15 0.996 1.0\n",
      "0.16 0.996 1.0\n",
      "0.17 0.996 1.0\n",
      "0.18 0.996 1.0\n",
      "0.19 0.996 1.0\n",
      "0.2 0.996 1.0\n",
      "0.21 0.996 1.0\n",
      "0.22 0.996 1.0\n",
      "0.23 0.996 1.0\n",
      "0.24 0.996 1.0\n",
      "0.25 0.994 1.0\n",
      "0.26 0.992 1.0\n",
      "0.27 0.992 1.0\n",
      "0.28 0.989 1.0\n",
      "0.29 0.989 0.996\n",
      "0.3 0.989 0.994\n",
      "0.31 0.983 0.994\n",
      "0.32 0.983 0.992\n",
      "0.33 0.979 0.99\n",
      "0.34 0.971 0.987\n",
      "0.35 0.971 0.987\n",
      "0.36 0.969 0.987\n",
      "0.37 0.967 0.981\n",
      "0.38 0.96 0.973\n",
      "0.39 0.954 0.967\n",
      "0.4 0.941 0.96\n",
      "0.41 0.91 0.95\n",
      "0.42 0.906 0.941\n",
      "0.43 0.885 0.924\n",
      "0.44 0.88 0.916\n",
      "0.45 0.86 0.902\n",
      "0.46 0.836 0.88\n",
      "0.47 0.826 0.87\n",
      "0.48 0.772 0.837\n",
      "0.49 0.757 0.824\n",
      "0.5 0.736 0.799\n",
      "0.51 0.686 0.742\n",
      "0.52 0.669 0.711\n",
      "0.53 0.635 0.679\n",
      "0.54 0.627 0.665\n",
      "0.55 0.61 0.656\n",
      "0.56 0.57 0.606\n",
      "0.57 0.564 0.579\n",
      "0.58 0.514 0.537\n",
      "0.59 0.47 0.511\n",
      "0.6 0.455 0.491\n",
      "0.61 0.407 0.426\n",
      "0.62 0.394 0.415\n",
      "0.63 0.365 0.394\n",
      "0.64 0.342 0.365\n",
      "0.65 0.298 0.34\n",
      "0.66 0.277 0.319\n",
      "0.67 0.262 0.306\n",
      "0.68 0.258 0.3\n",
      "0.69 0.22 0.26\n",
      "0.7 0.214 0.254\n",
      "0.71 0.214 0.254\n",
      "0.72 0.193 0.237\n",
      "0.73 0.178 0.222\n",
      "0.74 0.17 0.216\n",
      "0.75 0.164 0.208\n",
      "0.76 0.153 0.185\n",
      "0.77 0.138 0.172\n",
      "0.78 0.132 0.161\n",
      "0.79 0.098 0.126\n",
      "0.8 0.092 0.119\n",
      "0.81 0.092 0.119\n",
      "0.82 0.08 0.115\n",
      "0.83 0.073 0.094\n",
      "0.84 0.063 0.075\n",
      "0.85 0.052 0.067\n",
      "0.86 0.038 0.044\n",
      "0.87 0.034 0.04\n",
      "0.88 0.033 0.036\n",
      "0.89 0.027 0.033\n",
      "0.9 0.017 0.015\n",
      "0.91 0.01 0.01\n",
      "0.92 0.002 0.006\n",
      "0.93 0.0 0.002\n",
      "0.94 0.0 0.0\n",
      "0.95 0.0 0.0\n",
      "0.96 0.0 0.0\n",
      "0.97 0.0 0.0\n",
      "0.98 0.0 0.0\n",
      "0.99 0.0 0.0\n",
      "1.0 0.0 0.0\n",
      "is_abbr\n",
      "0.0 1.0 1.0\n",
      "0.01 1.0 1.0\n",
      "0.02 1.0 1.0\n",
      "0.03 1.0 1.0\n",
      "0.04 1.0 1.0\n",
      "0.05 1.0 1.0\n",
      "0.06 0.99 1.0\n",
      "0.07 0.986 1.0\n",
      "0.08 0.986 1.0\n",
      "0.09 0.986 1.0\n",
      "0.1 0.976 1.0\n",
      "0.11 0.976 1.0\n",
      "0.12 0.967 1.0\n",
      "0.13 0.967 1.0\n",
      "0.14 0.967 1.0\n",
      "0.15 0.962 1.0\n",
      "0.16 0.957 1.0\n",
      "0.17 0.952 1.0\n",
      "0.18 0.952 1.0\n",
      "0.19 0.952 1.0\n",
      "0.2 0.948 1.0\n",
      "0.21 0.943 1.0\n",
      "0.22 0.943 1.0\n",
      "0.23 0.943 1.0\n",
      "0.24 0.929 1.0\n",
      "0.25 0.929 1.0\n",
      "0.26 0.91 1.0\n",
      "0.27 0.905 1.0\n",
      "0.28 0.905 1.0\n",
      "0.29 0.895 1.0\n",
      "0.3 0.89 1.0\n",
      "0.31 0.876 1.0\n",
      "0.32 0.871 1.0\n",
      "0.33 0.852 1.0\n",
      "0.34 0.819 1.0\n",
      "0.35 0.819 1.0\n",
      "0.36 0.81 1.0\n",
      "0.37 0.776 0.99\n",
      "0.38 0.757 0.986\n",
      "0.39 0.743 0.981\n",
      "0.4 0.719 0.967\n",
      "0.41 0.695 0.962\n",
      "0.42 0.676 0.962\n",
      "0.43 0.657 0.933\n",
      "0.44 0.648 0.933\n",
      "0.45 0.629 0.919\n",
      "0.46 0.59 0.905\n",
      "0.47 0.59 0.89\n",
      "0.48 0.552 0.843\n",
      "0.49 0.505 0.819\n",
      "0.5 0.5 0.81\n",
      "0.51 0.462 0.738\n",
      "0.52 0.424 0.705\n",
      "0.53 0.362 0.667\n",
      "0.54 0.357 0.667\n",
      "0.55 0.343 0.638\n",
      "0.56 0.286 0.605\n",
      "0.57 0.257 0.567\n",
      "0.58 0.219 0.495\n",
      "0.59 0.186 0.452\n",
      "0.6 0.171 0.443\n",
      "0.61 0.143 0.367\n",
      "0.62 0.143 0.348\n",
      "0.63 0.114 0.31\n",
      "0.64 0.114 0.295\n",
      "0.65 0.105 0.276\n",
      "0.66 0.095 0.257\n",
      "0.67 0.09 0.252\n",
      "0.68 0.086 0.243\n",
      "0.69 0.071 0.205\n",
      "0.7 0.067 0.2\n",
      "0.71 0.067 0.2\n",
      "0.72 0.052 0.186\n",
      "0.73 0.043 0.181\n",
      "0.74 0.043 0.171\n",
      "0.75 0.043 0.167\n",
      "0.76 0.043 0.138\n",
      "0.77 0.038 0.1\n",
      "0.78 0.033 0.095\n",
      "0.79 0.033 0.086\n",
      "0.8 0.033 0.081\n",
      "0.81 0.033 0.081\n",
      "0.82 0.033 0.081\n",
      "0.83 0.029 0.067\n",
      "0.84 0.024 0.067\n",
      "0.85 0.024 0.067\n",
      "0.86 0.014 0.038\n",
      "0.87 0.014 0.029\n",
      "0.88 0.01 0.029\n",
      "0.89 0.01 0.019\n",
      "0.9 0.005 0.019\n",
      "0.91 0.005 0.014\n",
      "0.92 0.005 0.01\n",
      "0.93 0.0 0.0\n",
      "0.94 0.0 0.0\n",
      "0.95 0.0 0.0\n",
      "0.96 0.0 0.0\n",
      "0.97 0.0 0.0\n",
      "0.98 0.0 0.0\n",
      "0.99 0.0 0.0\n",
      "1.0 0.0 0.0\n"
     ]
    }
   ],
   "source": [
    "type_list = [\"is_special\", \"is_repeat\", \"is_interesting_up\", \"is_up\", \"is_abbr\"]\n",
    "for t in type_list:\n",
    "    print(t)\n",
    "    t_generator = (round(x * 0.01,2) for x in range(0, 101))\n",
    "    grouped_multiple_with_t = grouped_multiple[grouped_multiple[t] == 1]\n",
    "    for t in t_generator:\n",
    "        print(t,round(grouped_multiple_with_t[grouped_multiple_with_t[\"perspective_result_on_pert\"] > t].shape[0] / grouped_multiple_with_t.shape[0],3),round(grouped_multiple_with_t[grouped_multiple_with_t[\"perspective_result_on_clean\"] > t].shape[0] / grouped_multiple_with_t.shape[0],3))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ebd1281c5f5acee8a7f8a313b2ca9c6213b17a23ada965c46f901f6b2a7edb99"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
